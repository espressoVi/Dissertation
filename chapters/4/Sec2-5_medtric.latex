In the previous section, we illustrated that most commonly used metrics do not 
align well with clinical practice. In this section, we propose a new metric 
designed to meet the established criteria and incorporate clinically desirable 
properties.

\subsubsection{Definition}

Given $\mathcal{P}$, consider an instance of prediction and label $\hat{z}_i, y_i$.
There are three sets of interest, $\hat{z}_i \cap y_i$, $y_i - \hat{z}_i$, and, 
$\hat{z}_i - y_i$, corresponding to correct predictions, missed predictions, and
extra predictions, respectively (see Fig \ref{fig:sets}). Although $y_i -
\hat{z}_i$ and $\hat{z}_i - y_i$ both consist of errors, the former generally
has worse clinical outcomes.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{images/4/Sets.png}
    \caption{\textbf{The partitions of interest for clinical evaluation.}
    Figure reproduced from \citet{myMedtric}.}
    \label{fig:sets}
\end{figure}

Since each category poses a unique clinical scenario, we score them as follows:
\begin{equation}
    [a_{i}]_j = \begin{cases}
                \frac{s_j}{n_j} & \text{if } c_j \in \hat{z}_i \cap y_i\\
                \frac{-s_j}{n_j} & \text{if } c_j \in y_i - \hat{z}_i\\
                \frac{s_j}{n^{*}} \Big[\frac{1}{|y_i|}\Big(\sum_{c_k \in y_i} w_{jk}\Big) -1\Big] & \text{if } c_j \in \hat{z}_i - y_i\\
                0 & \text{otherwise}
    \end{cases}
	\label{eq:tcsm1}
\end{equation}
where $n_i$ is the number of occurrences of diagnostic condition $i$ in the dataset 
$\mathcal{P}$. This normalization ensures that the prevalence of diagnostic
conditions doesn't affect the final scores. $n^*$ is defined as follows:
\begin{equation}
    n^* = \max \{n_j | \forall c_j \in y_i \}    
\end{equation}
$\{s_j| \forall j \in \{1\ldots P \}\}$ are significance weights. This reflects 
the fact that all diagnostic conditions might not be equally relevant, and 
classes that are critical have a higher value of $s_i$, so their contribution 
to the final score is larger. They can all be set to 1 if their relative 
importance is the same.

$w_{jk}$ measures similarity of diagnostic conditions (as in \citet{cmpaper}). 
This gives partial rewards to over-diagnosis which are of similar nature in 
outcomes or treatment. If such a matrix is unavailable or not required, $w_{jk}$ 
can be set to $0$ $j\neq k$, and $w_{jj}=1\: \forall j$.
 
If, for a given dataset having $P$ conditions, all $2^P$ diagnoses are not 
possible, and contradictory pairs exist (hypo- and hypertension, for instance),
we can introduce an additional contradiction penalty term and a contradiction 
matrix $\mathrm{C}_{jk}$, such that $\mathrm{C}_{jk} = 1$ if condition 
$c_j$ and $c_k$ can't occur together ($\forall \text{ patient } x, x 
\text{ has } c_j \Rightarrow  x \text{ does not have } c_k$).
\begin{equation}
    [b_{i}]_j = \begin{cases}
                    \frac{-1}{ n_j} \sum_{\substack{\forall k \text{ s.t. } c_k \in \hat{z}_i}} s_k\cdot C_{jk} \text{ if } c_j \in \hat{z}_i\\
                    0 \text{, otherwise}
                \end{cases}
		\label{eq:tcsm2}
\end{equation}

Then we can compute the score for the $i^{th}$ instance as follows:
\begin{equation}
    t_i = \sum_{j = 1}^{P} [a_{i}]_j + [b_{i}]_j
\end{equation}

Finally, we sum the scores over all instances in the dataset and normalize. 
Consider $\mathrm{Y}, \mathrm{Z} = \{y_i | \forall i \in \{1,\ldots,N\}\}, 
\{\hat{z}_i | \forall i\in \{1,\ldots,N\}\}$,  we have $t(\mathrm{Y}, 
\mathrm{Z})$ defined as follows:
\begin{equation}
    t(\mathrm{Y}, \mathrm{Z}) = \sum_{i = 1}^{N} t_i
\end{equation}
\begin{equation}
    \label{eq:medtric-def}
    M_{med} = \frac{t(\mathrm{Y}, \mathrm{Z}) - t(\mathrm{Y}, \mathrm{\Phi})}{t(\mathrm{Y}, \mathrm{Y}) - t(\mathrm{Y}, \mathrm{\Phi})}   
\end{equation}
Here $\Phi$ represents the null prediction set, i.e., $\hat{z}_i = \phi, \forall 
i \in \{1 \ldots N\}$. This normalization ensures that a perfect classifier 
gets a maximum possible score of 1, and an inactive one that predicts nothing 
gets a score of 0. The metric defined in Equation \ref{eq:medtric-def} is named
\textbf{MedTric}, which is a portmanteau of \textbf{Med}ical Me\textbf{tric}.

\subsubsection{MedTric follows clinical order}

\begin{claim}
    MedTric always penalizes missed predictions more severely than extra predictions.
\end{claim}

\textit{Proof.} $\quad$ Since we have the following inequalities;
\begin{align*}
    0                  & < [w]_{jk}                                                                              < 1 &\quad\forall [w]_{jk}, j\neq k\\
    0                  & < \frac{1}{|y_i|}\sum_{c_k \in y_i} [w]_{jk}                                            < 1 &\\
    -\frac{s_j}{n_j}   & < \frac{s_j}{n^{*}} \Bigg[\frac{1}{|y_i|}\Big(\sum_{c_k \in y_i} [w]_{jk}\Big) -1\Bigg] < 0 &\\
    & &(n^* \geq n_j \:\forall j \in \{1 \ldots P\}, \text{ by definition})
\end{align*}
missed predictions always have heavier penalties than extra predictions.

This does not demonstrate that MedTric follows \ref{eq:order}, and since such a 
demonstration would be dependent on the exact clinical requirements and details 
about the dataset, we resort to empirical means in order to validate that 
\ref{eq:order} is maintained by MedTric. However, MedTric does have desirable 
behavior in most cases of practical interest. Consider (as in Section 
\ref{sec:labelbased}) 4 classifiers and their output sets $\mathcal{P}_O, 
\mathcal{P}_M, \mathcal{P}_W, \text{ and }\mathcal{P}$, corresponding to over, 
missed, wrong, and perfect diagnoses respectively, and a specific diagnostic 
condition $a_k$.

In $\mathcal{P}_M$ since only missed diagnoses are allowed, we have 
$fp_k = 0, \hat{z}_i - y_i = \phi$ and the assigned score is given by 
$\frac{s_k}{n_k}(tp_k - fn_k)$ where $tp_k, fp_k, fn_k$ are the number of true 
positives, false positives, and false negatives, respectively, for the condition 
$a_k$ in $\mathcal{P}_M$.

Similarly, in $\mathcal{P}_O$ since only over-diagnoses are allowed, we have 
$fn_k' = 0, y_i - \hat{z}_i = \phi, tp_k'=n_k$, and the assigned score is given 
by:
\begin{align*}
    & s_k + \sum_{a_k \notin y_i} \frac{s_k}{n^*}\Big[\frac{1}{|y_i|}\Big(\sum_{c_j \in y_i} w_{kj}\Big) -1\Big]\\
    &\geq s_k + \sum_{a_k \notin y_i} \frac{s_k}{n^*}\Big[\frac{1}{|y_i|}\Big(\sum_{c_j \in y_i} w_{k}^*\Big) -1\Big]\\
    & = s_k + \sum_{a_k \notin y_i} \frac{s_k}{n^*}\Big[\Big(w_{k}^*\Big) -1\Big]\\
    & = s_k + fp_k' \frac{s_k}{n^*}\Big(w_{k}^*-1\Big)\\
    \text{Where } w_{k}^* &= \min(w_{kj}\: \forall j \in \{1, \ldots, P\})
\end{align*}
Where, $tp_k', fp_k', fn_k'$ are the number of true positives, false positives,
and false negatives, respectively, for the condition $a_k$ in $\mathcal{P}_O$. 
Consider,
\begin{align*}
    \xi_k &= \overbrace{\frac{s_k}{n_k}(tp_k - fn_k)}^{\text{Missed diagnosis score}} - \overbrace{s_k - fp_k' \frac{s_k}{n^*}\Big(w_{k}^*-1\Big)}^{\text{Over-diagnosis score}}\\
        &=\frac{s_k}{n_k}\Big[ tp_k - fn_k - n_k - fp_k'\frac{n_k}{n^*}\Big(w_{k}^*-1\Big)\Big] (\text{ as, } tp_k + fn_k = n_k)\\
        &=\frac{s_k}{n_k}\Big[ fp_k'\frac{n_k}{n^*}\Big(1-w_{k}^*\Big) -2 \cdot fn_k \Big]\\
\end{align*}
Now, if $\xi_k < 0\;\forall k \in \{1,\ldots,P\}$, MedTric follows 
\ref{eq:order}. Even conservatively, since we have $\frac{n_k}{n^*} \leq 1$ and 
$0< 1-w_k^* < 1$ by definition, $\xi_k < 0$ holds whenever the number of false 
positives of each condition does not exceed twice the number of false negatives.

If a broader region of operation is required, $w_k^*$ can be adjusted 
accordingly, e.g., if $w_k^* = \frac{1}{3} \:\forall k$, MedTric follows 
\ref{eq:order} whenever the number of false positives of each condition does 
not exceed thrice the number of false negatives. In more realistic scenarios,
however, where prevalence is imbalanced, the region where \ref{eq:order} holds 
is much broader. For example, if a certain diagnostic condition is a tenth as 
likely as the most frequent one, we have $\xi_k < 0$ whenever the number of 
false positives for the condition is less than 20 times the number of false 
negatives for that same condition.

For $\mathcal{P}_W$, we have $tp_k = 0, fn_k = n_k, \hat{z}_i \cap y_i = \phi$,
and the score corresponding to $a_k$ is given by 
$-s_k -fp_k' \frac{s_k}{n^*}\Big(1-w_{k}^*\Big) \leq -s_k$, which is the lowest 
possible missed diagnosis score.

Thus, depending on the clinical context and its associated tolerance for missed 
diagnosis vs. over-diagnosis, we can choose the values of $w_{jk}$ such that 
MedTric is guaranteed to follow \ref{eq:order} (see example in Table 
\ref{tab:csm-tab2}).

\begin{table}[!ht]
	\caption{
        \textbf{Example of scoring for missed, over and wrong diagnoses.}\\ 
        O, M, W, P stands for over, missed, wrong and perfect diagnoses, 
        respectively. The following subscript number represents the quantity,
        e.g., $O_1$ means one over-diagnosis. \emph{MedTric} sorts them in the
        desired clinical order (labels are drawn from PhysioNet dataset).
    }
    \label{tab:csm-tab2}
    \begin{center}
        \begin{tabular}{ l l l c }
        \toprule
        \textbf{Ground Truth} & \textbf{Prediction} & \textbf{Type} & \textbf{Score} \\
        \midrule
            CRBBB, AF, QAb & LAD, STach, TInv & $W_3$ &-0.230\\
            CRBBB, AF, QAb & LAD, STach & $W_2$ &-0.159\\
            CRBBB, AF, QAb & LAD & $W_1$ &-0.081\\
            CRBBB, AF, QAb & $\phi$ & $M_3$ & 0.0\\
            CRBBB, AF, QAb & CRBBB & $M_2$ & 0.25\\
            CRBBB, AF, QAb & CRBBB, AF & $M_1$ & 0.75\\
            CRBBB, AF, QAb & CRBBB, AF, QAb, LAD, NSIVCB & $O_2$ & 0.756 \\
            CRBBB, AF, QAb & CRBBB, AF, QAb, LAD & $O_1$ & 0.918 \\
            CRBBB, AF, QAb & CRBBB, AF, QAb & $P$ & 1 \\
        \bottomrule
        \end{tabular}
    \end{center}
\end{table}


\subsubsection{Dataset Artifacts}

\begin{table}[!ht]
	\caption{
        \textbf{Example illustrating dataset prevalence independence.}\\ 
        Here in the two cases shown above, the underlying classification quality
        is the same; conditions A and B are detected 100\% of the time, and
        condition X is detected 50\% of the time; only the prevalence in the
        dataset has changed (in Case 1, $\{X, A\}$ occurs 10\% of the time and
        in Case 2, 90\% of the time). However, unlike other metrics (e.g., F1
        score), this doesn't change the MedTric score, thus demonstrating
        dataset prevalence invariance.
    }
    \begin{center}
    \begin{tabular}{l|l c r|l|r r}
        \toprule
        \textbf{Prediction}$\rightarrow$ & $\{X, A\}$ &  $\{A\}$ & $\{A,B\}$ & \textbf{Total} & $\mathbf{F_1}$ & \textbf{MedTric} \\
        \midrule
        \tikzmark{a1}\textbf{GT:} $\{X, A\}$ & 50  & 50  & 0   & 100 & \multirow{2}{*}{0.983} & \multirow{2}{*}{0.833} \\
        \tikzmark{a2}\textbf{GT:} $\{A,B\}$  & 0   &  0  & 900 & 900 & & \\
        \hline\hline
        \tikzmark{b1}\textbf{GT:} $\{X, A\}$ & 450 & 450 & 0   & 900 & \multirow{2}{*}{0.850} & \multirow{2}{*}{0.833}\\
	    \tikzmark{b2}\textbf{GT:} $\{A,B\}$  & 0   &  0  & 100 & 100 & & \\
        \bottomrule
    \end{tabular}
    \end{center}
    \begin{tikzpicture}[overlay, remember picture]
        \draw [decorate,decoration={brace,amplitude=10pt,mirror,raise=4pt}] (a1.north) --node[left=14pt]{Case 1} (a2.south);
        \draw [decorate,decoration={brace,amplitude=10pt,mirror,raise=4pt}] (b1.north) --node[left=14pt]{Case 2} (b2.south);
    \end{tikzpicture}
    \label{tab:csm-indepEx}
\end{table}
If a computational system is X\% accurate in one diagnostic class and Y\% in 
another, some metrics may change solely due to variations in the proportions of 
these classes. Micro-averaged label-based metrics and example-based metrics are 
particularly vulnerable to this issue. Evaluating model performance can be 
obscured by demographic artifacts, especially given the common issue of class 
imbalance in diagnostic datasets \cite{csl-method}.

To address this problem, we suggest normalizing each score contribution by the 
corresponding class frequency (see Equations \ref{eq:tcsm1} and \ref{eq:tcsm2}), 
ensuring that the final score is independent of dataset proportions and 
represents the true per-instance accuracy (refer to Table \ref{tab:csm-indepEx}).

Our proposal aligns well with the principles of cost-sensitive learning 
\cite{csl}. As discussed in the preceding section, our metric imposes a greater 
penalty for false negatives (missed diagnoses) than for false positives 
(over-diagnosis). Furthermore, we have disentangled the prevalence of diagnostic 
conditions from performance measures, recognizing that rarity does not 
necessarily equate to severity.

Beyond prevalence, diagnostic datasets often incorporate a notion of 
criticality, which is not typically reflected in standard machine-learning 
metrics. This aspect of criticality necessitates an additional layer of 
cost-based decision-making. The significance weights $s_j$ (see Equations 
\ref{eq:tcsm1} and \ref{eq:tcsm2}) ensure that classifiers that underperform on 
critical classes face harsher penalties. These weights are normalized so they 
can be interpreted as the \emph{contribution of a particular diagnostic class 
to the final score}.

Additionally, by introducing $w_{jk}$ and $C_{jk}$, we account for interactions 
between different diagnostic classes in a manner that is informed by domain 
knowledge. For instance, if two diagnostic conditions have similar prognoses or 
treatment plans, misclassifying one as the other might be penalized less 
severely \cite{cmpaper}. This framework can be interpreted as a cost-sensitive 
learning problem with a cost matrix \( \mathrm{M} \in \mathbb{R}^{2^P \times 
2^P} \) where every misclassification of a set \( \alpha \in 2^A \) as another 
set \( \beta \in 2^A \) carries a potentially distinct cost.


