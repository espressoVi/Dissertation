\begin{figure}[!ht]
    \begin{center}
        \begin{subfigure}[l]{\textwidth}
            \centering
            \includegraphics[width=0.95\textwidth]{images/3/cls1.png}
            \caption{\textbf{OS} and \textbf{TS}}
        \end{subfigure}
        \begin{subfigure}[l]{\textwidth}
            \centering \includegraphics[width=0.95\textwidth]{images/3/cls2.png}
            \caption{\textbf{OS} and \textbf{TS-R}}
        \end{subfigure}
    \end{center}
    \caption{
        \textbf{Differences between embeddings for OT and IBT models.}\\
        We plot the histogram of cosine similarity between the \textbf{[CLS]}
        embedding produced by a model trained with the \textbf{OT} and 
        \textbf{IBT} strategy under different intervention schemes. Results are
        reported with RoBERTa-large, and the figure is reproduced from \citet{myCL}.
    }
    \label{fig:cls}
\end{figure}

We perform further experiments to analyze the efficacy of the \textbf{IBT}
training strategy with deletion intervention. To this end, we employ the 
cosine similarity ($cossim$) measure on the output embeddings produced by the
models on the various intervention-based datasets. The $cossim$ of output 
embeddings is an informative measure since embeddings with high $cossim$ are 
extremely likely to be treated similarly by the downstream \ac{FC} layer, thus
leading to similar final model predictions. We first analyze this with the
final layer $[CLS]$ embeddings, as they represent summarized contextual 
information necessary for classification. We measure the $cossim$ between
embeddings produced by the model on the \textbf{TS} and \textbf{TS-R} datasets
against the unmodified \textbf{OS} dataset. Figure \ref{fig:cls} plots the
histogram of $cossim$ values for embeddings generated by RoBERTa-large. The
sharply peaked distribution at $1$ in Figure \ref{fig:cls}(a) indicates that
there aren't major deviations from the \textbf{OS} dataset for the \textbf{TS}
intervention following either training strategy; however, there is a stark 
difference between \textbf{OS} and \textbf{TS-R} (shown in Figure \ref{fig:cls}(b))
with the two training strategies. The drop in $cossim$ observed in Figure 
\ref{fig:cls}(b) indicates that the model modifies its embeddings in response 
to critical information deletion but maintains them despite large chunks of 
``irrelevant'' information being removed, which is the desired behavior. This 
trend is not localized to the $[CLS]$ embedding, but a similar experiment 
working with every common token in \textbf{OS, TS}, and \textbf{TS-R} shows the 
same behavior (see Figure \ref{fig:cos}). This indicates that the \textbf{IBT}
strategy successfully makes models more aligned to domain expectations while 
maintaining baseline performance.

\begin{figure}[!ht]
    \begin{center}
        \begin{subfigure}[l]{\textwidth}
            \centering
            \includegraphics[width=0.95\textwidth]{images/3/cos1.png}
            \caption{\textbf{OS} and \textbf{TS}}
        \end{subfigure}
        \begin{subfigure}[l]{\textwidth}
            \centering
            \includegraphics[width=0.95\textwidth]{images/3/cos2.png}
            \caption{\textbf{OS} and \textbf{TS-R}}
        \end{subfigure}
    \end{center}
    \caption{
        \textbf{Differences between embeddings for OT and IBT models.}\\
        We plot the histogram of cosine similarity between the common token 
        embeddings produced by a model trained with the \textbf{OT} and 
        \textbf{IBT} strategy under different intervention schemes. Results are
        reported with RoBERTa-large, and the figure is reproduced from \citet{myCL}.
    }
    \label{fig:cos}
\end{figure}
