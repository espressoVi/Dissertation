During the course of tackling a crossword puzzle, solvers often deal with 
partly-filled grids, where solutions to some clues have already been unearthed. 
These intermediate states provide valuable hints that solvers strategically use 
to guide their choices for the remaining answers. For instance, consider Figure
\ref{fig:crosswordFig}: when attempting to solve the clue in position 14
(across), one can leverage the letters already determined from position 2 (down)
and 4 (down). This narrows the possibilities for the answer to those that match
the pattern ``\texttt{\_ T \_ P}''. In this section, we examine the capabilities
of \acp{LLM} in utilizing such constraints to improve their responses. Our
prompting strategy for this task is similar to the \ac{QA} task in the preceding
section, but we additionally provide \emph{``letter masks''}, which are supposed
to serve as constraining information in order to guide generation. When creating
a query for a particular test instance with $k\%$ hints, we randomly selected
$N$ few-shot instances and ensured the few-shot examples also had $k\%$ hints.
The number of characters revealed $(h)$ is given by the formula:
\begin{equation}
    h = \max\Bigg(1, \text{round}\Big(\frac{k}{100}\times len(\text{answer})\Big)\Bigg)\: \forall k>0
\end{equation}
$h$ many characters are randomly selected and revealed, all other characters are
replaced with ``\texttt{\_}''. The prompt structure is given in Figure \ref{fig:partclcs}.
\begin{figure}[!ht]
    \footnotesize{
    \begin{tcolorbox}[colback=boxcol,colframe=black]
        \begin{verbatim}
[{ "role":"user", 
    "content": "Clue: <clue 1> (3) // _ _ N => MEN\n
                Clue: < ...  > (6) // _ _ E _ _ _ => BREATH\n
                Clue: <query clue> (length) // _ _ X _ X => "
}] \end{verbatim}
    \end{tcolorbox}}
    \caption{Prompt used to solve crossword clues with character hints.}
    \label{fig:partclcs}
\end{figure}

For this experiment, we selected the top-performing open weights and proprietary 
models, specifically LLaMA 3 70B and GPT-4-Turbo. Additionally, we included 
results from smaller models to assess whether the observed trends persist across 
model sizes. The evaluation was conducted on the NYT and \emph{Init} datasets 
using 5-shot prompts. In each query, $k$\% of the answer's characters are 
provided alongside the clue and the expected length of the answer. The \acp{LLM}
are tasked with ``unmasking'' the remaining characters by leveraging the given 
constraints and the crossword clue. Our results are summarized in Table 
\ref{tab:hinted_results}.

\begin{table}[!ht]
    \caption{
        \textbf{Can LLMs exploit character constraints from a partially filled grid?}\\
        \citet{llmcryptic} reported an accuracy of 27.0\% (70\% hinted clues) by 
        fine-tuning a Mistral 7B model on the \emph{Init} dataset, 
        \emph{which GPT-4-Turbo} (\textbf{76.30\%} accuracy) \emph{outperforms 
        by a factor of} \textbf{$\sim$2.8$\times$} \emph{without fine-tuning}.
        All results are with 5-shot prompts.
    }
    \label{tab:hinted_results}
    \begin{center}
        \footnotesize{
        \begin{tabular}[c]{l|l r|l r|l r|r}
            \toprule
            \multirow{2}{*}{Hint (\%)} & \multicolumn{2}{c|}{\textbf{0\%}} & \multicolumn{2}{c|}{\textbf{25\%}} & \multicolumn{2}{c|}{\textbf{50\%}}& \textbf{70\%} \\
                        \cline{2-3} \cline{4-5} \cline{6-7} \cline{8-8}
                         & NYT       & \emph{init}   & NYT   & \emph{init}   & NYT   & \emph{init} & \emph{init} \\
            \midrule
            Mistral 7B   & 10.95\%   & 1.70\%    & 9.70\%    & 2.80\%    & 11.95\%   & 4.80\%    & \\
            LlaMa 3 8B   & 15.8\%    & 1.30\%    & 19.7\%    & 2.85\%    & 24.65\%   & 6.25\%    & \\
            LlaMa 3 70B  & 27.20\%   & 6.40\%    & 31.80\%   & 11.45\%   & 45.30\%   & 20.35\%   & \\
            GPT 4 Turbo  & 41.2\%    & 18.70\%   & 59.95\%   & 33.70\%   & 75.75\%   & 52.85\%   & \textbf{76.30\%}\\
            \bottomrule
        \end{tabular}}
    \end{center}
\end{table}

As reflected in Table \ref{tab:hinted_results}, for both datasets under 
consideration, we observe that in nearly every scenario, \acp{LLM} demonstrate 
enhanced performance as the percentage of constraint information increases. 
Furthermore, to benchmark GPT-4-Turbo against the previously reported \ac{SoTA}
results by \citet{llmcryptic}, we conducted our experiment using their same 
settings and dataset split. The findings reveal that \emph{GPT-4-Turbo}, with 
5-shot prompts, achieves an accuracy of \textbf{76.3\%}, significantly 
surpassing the fine-tuned Mistral 7B model's \textbf{27\%} accuracy by a 
\textbf{factor of 2.8}. This ability of LLMs to effectively utilize constraints 
for deciphering crossword clues indicates that they are well-equipped for the 
comprehensive task of solving entire crosswords.
